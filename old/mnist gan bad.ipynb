{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# adapted (copy pasted) from https://github.com/znxlwm/pytorch-MNIST-CelebA-GAN-DCGAN\n",
    "import os, time\n",
    "import matplotlib.pyplot as plt\n",
    "# import itertools\n",
    "import pickle\n",
    "# import imageio\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "# from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-a29f555e4e1f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;31m# initializers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# changed things\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class discriminator(nn.Module):\n",
    "    def __init__(self, d=64):\n",
    "        super(discriminator, self).__init__()\n",
    "        # change all the 4,2,1) to 5,2) because that's how pacgan does it???\n",
    "        self.conv1 = nn.Conv2d(3, d, 4, 2, 1) # in_channel, out_channel, kernel size, stride, padding\n",
    "        self.conv2 = nn.Conv2d(d, d*2, 4, 2, 1)\n",
    "        self.conv2_bn = nn.BatchNorm2d(d*2)\n",
    "        self.conv3 = nn.Conv2d(d*2, d*4, 4, 2, 1)\n",
    "        self.conv3_bn = nn.BatchNorm2d(d*4)\n",
    "        self.conv4 = nn.Conv2d(d*4, d*8, 4, 2, 1)\n",
    "        self.conv4_bn = nn.BatchNorm2d(d*8)\n",
    "        self.conv5 = nn.Conv2d(d*8, 1, 4, 1, 0)\n",
    "    \n",
    "    # weight_init\n",
    "    def weight_init(self, mean, std):\n",
    "        for m in self._modules:\n",
    "            normal_init(self._modules[m], mean, std)\n",
    "\n",
    "    # forward method\n",
    "    def forward(self, input):\n",
    "        x = F.leaky_relu(self.conv1(input), 0.2)\n",
    "        x = F.leaky_relu(self.conv2_bn(self.conv2(x)), 0.2)\n",
    "        x = F.leaky_relu(self.conv3_bn(self.conv3(x)), 0.2)\n",
    "        x = F.leaky_relu(self.conv4_bn(self.conv4(x)), 0.2)\n",
    "        x = F.sigmoid(self.conv5(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# G(z)\n",
    "class generator(nn.Module):\n",
    "    # initializers\n",
    "    def __init__(self, d=128):\n",
    "        super(generator, self).__init__()\n",
    "        self.deconv1 = nn.ConvTranspose2d(100, d*8, 4, 1, 0)\n",
    "        self.deconv1_bn = nn.BatchNorm2d(d*8)\n",
    "        self.deconv2 = nn.ConvTranspose2d(d*8, d*4, 4, 2, 1)\n",
    "        self.deconv2_bn = nn.BatchNorm2d(d*4)\n",
    "        self.deconv3 = nn.ConvTranspose2d(d*4, d*2, 4, 2, 1)\n",
    "        self.deconv3_bn = nn.BatchNorm2d(d*2)\n",
    "        self.deconv4 = nn.ConvTranspose2d(d*2, d, 4, 2, 1)\n",
    "        self.deconv4_bn = nn.BatchNorm2d(d)\n",
    "        self.deconv5 = nn.ConvTranspose2d(d, 1, 4, 2, 1)\n",
    "\n",
    "    # weight_init\n",
    "    def weight_init(self, mean, std):\n",
    "        for m in self._modules:\n",
    "            normal_init(self._modules[m], mean, std)\n",
    "\n",
    "    # forward method\n",
    "    def forward(self, input):\n",
    "        # x = F.relu(self.deconv1(input))\n",
    "        x = F.relu(self.deconv1_bn(self.deconv1(input)))\n",
    "        x = F.relu(self.deconv2_bn(self.deconv2(x)))\n",
    "        x = F.relu(self.deconv3_bn(self.deconv3(x)))\n",
    "        x = F.relu(self.deconv4_bn(self.deconv4(x)))\n",
    "        x = F.tanh(self.deconv5(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class discriminator(nn.Module):\n",
    "    # initializers\n",
    "    def __init__(self, d=128):\n",
    "        super(discriminator, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, d, 4, 2, 1)\n",
    "        self.conv2 = nn.Conv2d(d, d*2, 4, 2, 1)\n",
    "        self.conv2_bn = nn.BatchNorm2d(d*2)\n",
    "        self.conv3 = nn.Conv2d(d*2, d*4, 4, 2, 1)\n",
    "        self.conv3_bn = nn.BatchNorm2d(d*4)\n",
    "        self.conv4 = nn.Conv2d(d*4, d*8, 4, 2, 1)\n",
    "        self.conv4_bn = nn.BatchNorm2d(d*8)\n",
    "        self.conv5 = nn.Conv2d(d*8, 1, 4, 1, 0)\n",
    "\n",
    "    # weight_init\n",
    "    def weight_init(self, mean, std):\n",
    "        for m in self._modules:\n",
    "            normal_init(self._modules[m], mean, std)\n",
    "\n",
    "    # forward method\n",
    "    def forward(self, input):\n",
    "        x = F.leaky_relu(self.conv1(input), 0.2)\n",
    "        x = F.leaky_relu(self.conv2_bn(self.conv2(x)), 0.2)\n",
    "        x = F.leaky_relu(self.conv3_bn(self.conv3(x)), 0.2)\n",
    "        x = F.leaky_relu(self.conv4_bn(self.conv4(x)), 0.2)\n",
    "        x = F.sigmoid(self.conv5(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normal_init(m, mean, std):\n",
    "    if isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Conv2d):\n",
    "        m.weight.data.normal_(mean, std)\n",
    "        m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fixed_z_ = torch.randn((5 * 5, 100)).view(-1, 100, 1, 1)    # fixed noise\n",
    "# fixed_z_ = Variable(fixed_z_.cuda(), volatile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# training parameters\n",
    "batch_size = 128\n",
    "lr = 0.0002\n",
    "train_epoch = 20\n",
    "\n",
    "# data_loader\n",
    "# img_size = 64\n",
    "# transform = transforms.Compose([\n",
    "#         transforms.Scale(img_size),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "# ])\n",
    "# train_loader = torch.utils.data.DataLoader(\n",
    "#     datasets.MNIST('data', train=True, download=True, transform=transform),\n",
    "#     batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f.read(16)\n",
    "buffer = f.read(hw * n)\n",
    "images = np.frombuffer(buffer, dtype=np.uint8)\n",
    "images = images.reshape(n, hw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# network\n",
    "G = generator(128)\n",
    "D = discriminator(128)\n",
    "G.weight_init(mean=0.0, std=0.02)\n",
    "D.weight_init(mean=0.0, std=0.02)\n",
    "# G.cuda()\n",
    "# D.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Binary Cross Entropy loss\n",
    "BCE_loss = nn.BCELoss()\n",
    "\n",
    "# Adam optimizer\n",
    "G_optimizer = optim.Adam(G.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "D_optimizer = optim.Adam(D.parameters(), lr=lr, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# results save folder\n",
    "if not os.path.isdir('MNIST_DCGAN_results'):\n",
    "    os.mkdir('MNIST_DCGAN_results')\n",
    "if not os.path.isdir('MNIST_DCGAN_results/Random_results'):\n",
    "    os.mkdir('MNIST_DCGAN_results/Random_results')\n",
    "if not os.path.isdir('MNIST_DCGAN_results/Fixed_results'):\n",
    "    os.mkdir('MNIST_DCGAN_results/Fixed_results')\n",
    "\n",
    "train_hist = {}\n",
    "train_hist['D_losses'] = []\n",
    "train_hist['G_losses'] = []\n",
    "train_hist['per_epoch_ptimes'] = []\n",
    "train_hist['total_ptime'] = []\n",
    "num_iter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1b0aae08048>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x[0].shape\n",
    "%matplotlib inline\n",
    "plt.imshow(x[0][0,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25, 100, 1, 1])\n",
      "torch.Size([25, 1024, 4, 4])\n",
      "torch.Size([25, 512, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "print(fixed_z_.shape)\n",
    "print(G.deconv1(fixed_z_).shape)\n",
    "print(G.deconv2(G.deconv1(fixed_z_)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\owner\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:995: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([25, 1, 64, 64])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G(fixed_z_).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generator(\n",
       "  (deconv1): ConvTranspose2d(100, 1024, kernel_size=(4, 4), stride=(1, 1))\n",
       "  (deconv1_bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (deconv2): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (deconv2_bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (deconv3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (deconv3_bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (deconv4): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (deconv4_bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (deconv5): ConvTranspose2d(128, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "discriminator(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (conv2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (conv2_bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (conv3_bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (conv4_bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv5): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 512, 1, 1])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = torch.zeros(50,3,28,28)\n",
    "D = discriminator(64)\n",
    "D.conv1(image).shape, D.conv2(D.conv1(image)).shape, D.conv3(D.conv2(D.conv1(image))).shape\n",
    "D.conv4(D.conv3(D.conv2(D.conv1(image)))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python.framework import ops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(input_, output_dim, \n",
    "       k_h=5, k_w=5, d_h=2, d_w=2, stddev=0.02,\n",
    "       name=\"conv2d\"):\n",
    "  with tf.variable_scope(name):\n",
    "    w = tf.get_variable('w', [k_h, k_w, input_.get_shape()[-1], output_dim],\n",
    "              initializer=tf.truncated_normal_initializer(stddev=stddev))\n",
    "    conv = tf.nn.conv2d(input_, w, strides=[1, d_h, d_w, 1], padding='SAME')\n",
    "\n",
    "    biases = tf.get_variable('biases', [output_dim], initializer=tf.constant_initializer(0.0))\n",
    "    conv = tf.reshape(tf.nn.bias_add(conv, biases), conv.get_shape())\n",
    "\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Input 'filter' of 'Conv2D' Op has type float32 that does not match type float64 of argument 'input'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\owner\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    509\u001b[0m                 \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_ref\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 510\u001b[1;33m                 preferred_dtype=default_dtype)\n\u001b[0m\u001b[0;32m    511\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\owner\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, ctx)\u001b[0m\n\u001b[0;32m   1143\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1144\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\owner\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\u001b[0m in \u001b[0;36m_TensorConversionFunction\u001b[1;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[0;32m    798\u001b[0m           \u001b[1;34m\"Incompatible type conversion requested to type '%s' for variable \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 799\u001b[1;33m           \"of type '%s'\" % (dtype.name, v.dtype.name))\n\u001b[0m\u001b[0;32m    800\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Incompatible type conversion requested to type 'float64' for variable of type 'float32_ref'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-fb07e5a8ab5b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mconv2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-30-2b3b28949368>\u001b[0m in \u001b[0;36mconv2d\u001b[1;34m(input_, output_dim, k_h, k_w, d_h, d_w, stddev, name)\u001b[0m\n\u001b[0;32m      5\u001b[0m     w = tf.get_variable('w', [k_h, k_w, input_.shape[-1], output_dim],\n\u001b[0;32m      6\u001b[0m               initializer=tf.truncated_normal_initializer(stddev=stddev))\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mconv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'SAME'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mbiases\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'biases'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0moutput_dim\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\owner\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[1;34m(input, filter, strides, padding, use_cudnn_on_gpu, data_format, dilations, name)\u001b[0m\n\u001b[0;32m   1041\u001b[0m         \u001b[1;34m\"Conv2D\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1043\u001b[1;33m         data_format=data_format, dilations=dilations, name=name)\n\u001b[0m\u001b[0;32m   1044\u001b[0m     \u001b[0m_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1045\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\owner\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    544\u001b[0m                   \u001b[1;34m\"%s type %s of argument '%s'.\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    545\u001b[0m                   (prefix, dtypes.as_dtype(attrs[input_arg.type_attr]).name,\n\u001b[1;32m--> 546\u001b[1;33m                    inferred_from[input_arg.type_attr]))\n\u001b[0m\u001b[0;32m    547\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m           \u001b[0mtypes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Input 'filter' of 'Conv2D' Op has type float32 that does not match type float64 of argument 'input'."
     ]
    }
   ],
   "source": [
    "image = np.zeros((50,28,28,3))\n",
    "conv2d(image,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
